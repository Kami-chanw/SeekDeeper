[\[ğŸ“–English ReadMe\]](./README.md)
## Introduction
åœ¨è¿™é‡Œï¼Œæˆ‘å®ç°äº†ä¸€ä¸ªTransformerï¼Œå¹¶ä½¿ç”¨å…¶Encoderåœ¨IMDBæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼ˆè§[æ­¤](./train_imdb.ipynb)ï¼‰ã€‚

## Model details
### [Transformer](./modules/transformer.py)
Transformeræœ€åˆæå‡ºè¢«ç”¨äºè§£å†³ç¿»è¯‘ä»»åŠ¡ã€‚å¦‚æœè¦å®ç°ä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç§°ä¸­æ–‡ä¸ºæºè¯­è¨€ï¼Œè‹±æ–‡ä¸ºç›®æ ‡è¯­è¨€ã€‚Transformerçš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæºæ–‡æœ¬çš„embeddingä¸positional encodingç›¸åŠ åè¾“å…¥åˆ°Encoderï¼Œç»è¿‡Nå±‚Encoder layeråï¼Œè¾“å‡ºåœ¨Decoderçš„cross attentionä¸­è¿›è¡Œäº¤äº’ã€‚ç›®æ ‡æ–‡æœ¬çš„embeddingåŒæ ·ä¸positional encodingç›¸åŠ åè¾“å…¥åˆ°Decoderï¼ŒDncoderçš„è¾“å‡ºé€šå¸¸ä¼šå†ç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆå…·ä½“å–å†³äºä»»åŠ¡è¦æ±‚ï¼‰ã€‚
<div style="text-align: center;">
  <img src="./images/transformer.png" alt="Transformer" style="width: 300px; height: auto;">
</div>

Encoderå’ŒDecoderåˆ†åˆ«ä½¿ç”¨äº†ä¸¤ç§maskï¼Œ`src_mask`å’Œ`tgt_mask`ã€‚`src_mask`ç”¨äºé®ç›–æ‰€æœ‰çš„PAD tokenï¼Œé¿å…å®ƒä»¬åœ¨attentionè®¡ç®—ä¸­äº§ç”Ÿå½±å“ã€‚`tgt_mask`é™¤äº†é®ç›–æ‰€æœ‰PAD tokenï¼Œè¿˜è¦é˜²æ­¢æ¨¡å‹åœ¨è¿›è¡Œnext word predictionæ—¶è®¿é—®æœªæ¥çš„è¯ã€‚

### [Positional Encoding](./modules/layers.py)
ç”±äºTransformerä¸åƒRNNé‚£æ ·å…·æœ‰å¤©ç„¶çš„åºåˆ—ç‰¹æ€§ï¼Œåœ¨è®¡ç®—attentionæ—¶ä¼šä¸¢å¤±é¡ºåºä¿¡æ¯ï¼Œå› æ­¤éœ€è¦å¼•å…¥ä½ç½®ç¼–ç ã€‚åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œä½ç½®ç¼–ç çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

- å¯¹äºå¶æ•°ç»´åº¦ï¼š
  $$
   \text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
  $$

- å¯¹äºå¥‡æ•°ç»´åº¦ï¼š
  $$ 
  \text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) 
  $$

ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯¹div termå–æŒ‡æ•°å’Œå¯¹æ•°ï¼Œå³ï¼š
$$
\text{div\_term} = 10000^{2i/d_{\text{model}}} = \exp\left(\frac{2i \cdot -\log(10000)}{d_{\text{model}}}\right)
$$

ä½ç½®ç¼–ç å¯¹ä»»ä½•åºåˆ—éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤positional encodingçš„shapeä¸º`[seq_len, d_model]`ã€‚ç„¶åæ ¹æ®å¹¿æ’­æœºåˆ¶ä¸shapeä¸º`[batch_size, seq_len, d_model]`çš„input embeddingç›¸åŠ ï¼Œå¾—åˆ°Encoderçš„è¾“å…¥ï¼Œè®°ä½œ$x_0$ã€‚

## [Encoder](./modules/encoder.py)
EncoderåŒ…å«å¤šä¸ªç›¸åŒçš„å±‚ã€‚ä¸Šä¸€å±‚çš„è¾“å‡º$x_i$ä»¥å¦‚ä¸‹é€”å¾„ç»è¿‡è¯¥å±‚ï¼ˆçœç•¥äº†dropoutï¼‰ï¼š
```python
# attention mechanism
residual = x
x = multihead_attention(q=x, k=x, v=x, mask=src_mask)
x = layer_norm(x + residual)

# position-wise feed forward
residual = x
x = feed_forward(x)
x = layer_norm(x + residual)
```

## [Attention](./modules/layers.py)
Attentionçš„è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š
<div style="text-align: center;">
  <img src="./images/attention.png" alt="Attention" style="width: 400px; height: auto;">
</div>
åœ¨Encoderçš„self-attentionä¸­ï¼ŒKã€Qã€Vå‡ä¸ºä¸Šä¸€å±‚çš„è¾“å‡ºç»è¿‡ä¸åŒçº¿æ€§å±‚å¾—åˆ°çš„ã€‚åœ¨Decoderçš„cross-attentionä¸­ï¼ŒKå’ŒVæ¥è‡ªEncoderæœ€åä¸€å±‚çš„è¾“å‡ºï¼Œè€ŒQæ˜¯Decoderä¸Šä¸€å±‚çš„è¾“å‡ºã€‚

ä¸ºäº†ä½¿æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„ä¸åŒç‰¹å¾å­ç©ºé—´ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå°†shapeä¸º`[batch_size, seq_len, d_model]`çš„Kã€Qã€Våˆ†ä¸º`[batch_size, seq_len, n_head, d_key]`ï¼Œå†äº¤æ¢`seq_len`å’Œ`n_head`ä¸¤ä¸ªç»´åº¦ï¼Œä»¥ä¾¿è¿›è¡Œattentionæœºåˆ¶ä¸­çš„çŸ©é˜µä¹˜æ³•ã€‚è®¡ç®—äº†attentionä¹‹åå†å°†ç»“æœåˆå¹¶ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°ä¸è¾“å…¥ç›¸åŒçš„ç»´åº¦ã€‚ç®—æ³•çš„æµç¨‹å¦‚ä¸‹ï¼š
```python
# projection
K, Q, V = W_k(x), W_q(x), W_v(x)

# split
d_key = d_model // n_head
K, Q, V = (K, Q, V).view(batch_size, seq_len, n_head, d_key).transpose(1, 2)
out = scaled_dot_product_attention(K, Q, V)

#concatenate
out = out.transpose(1, 2).view(batch_size, seq_len, d_model)
out = W_cat(out)
```

Scaled Dot-Product Attentionç”¨å…¬å¼è¡¨ç¤ºä¸ºï¼š
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_{key}}}\right) \cdot V
$$

## [Decoder](./modules/decoder.py)
Decoderç›¸è¾ƒäºEncoderé™¤äº†å¤šäº†ä¸€å±‚cross-attentionä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨äº†masked multi-head attentionã€‚ç”±äºæ¨¡å‹åœ¨æ­¤å¤„ä¸èƒ½è®¿é—®æœªæ¥ä¿¡æ¯ï¼Œå› æ­¤è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿç§°ä¸ºcausal self-attentionã€‚
DecoderåŒæ ·åŒ…å«å¤šä¸ªç›¸åŒçš„å±‚ï¼ŒEncoderæœ€åä¸€å±‚çš„è¾“å‡º`enc`å’ŒDecoderä¸Šä¸€å±‚çš„è¾“å‡º`dec`ä»¥å¦‚ä¸‹é€”å¾„ç»è¿‡è¯¥å±‚ï¼ˆçœç•¥äº†dropoutï¼‰ï¼š
```python
# causal self-attention
residual = dec
x = multihead_attention(q=dec, k=dec, v=dec, mask=tgt_mask)
x = layer_norm(x + residual)

# cross-attention
x = multihead_attention(q=x, k=enc, v=enc, mask=tgt_mask)
x = layer_norm(x + residual)

# position-wise feed forward
residual = x
x = feed_forward(x)
x = layer_norm(x + residual)
```
