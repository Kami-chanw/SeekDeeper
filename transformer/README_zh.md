[ğŸ“–English ReadMe](./README.md)
## Introduction
åœ¨è¿™é‡Œï¼Œæˆ‘å®ç°äº†ä¸€ä¸ª Transformerï¼Œå¹¶ä½¿ç”¨å…¶åœ¨ IWSLT 2017 æ•°æ®é›†ä¸Šè¿›è¡Œäº†è‹±-å¾·ç¿»è¯‘ä»»åŠ¡ï¼ˆè§[æ­¤](./train.ipynb)ï¼‰ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œä½ å¯ä»¥åœ¨[æ­¤](./inference.ipynb)åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚

## Model details
### [Transformer](./modules/transformer.py)
Transformer æœ€åˆè¢«æå‡ºç”¨äºè§£å†³ç¿»è¯‘ä»»åŠ¡ã€‚å¦‚æœè¦å®ç°ä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç§°ä¸­æ–‡ä¸ºæºè¯­è¨€ï¼Œè‹±æ–‡ä¸ºç›®æ ‡è¯­è¨€ã€‚Transformer çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæºæ–‡æœ¬çš„ embedding ä¸ positional encoding ç›¸åŠ åè¾“å…¥åˆ° Encoderï¼Œç»è¿‡ N å±‚ Encoder layer åï¼Œè¾“å‡ºåœ¨ Decoder çš„ cross attention ä¸­è¿›è¡Œäº¤äº’ã€‚ç›®æ ‡æ–‡æœ¬çš„ embedding åŒæ ·ä¸ positional encoding ç›¸åŠ åè¾“å…¥åˆ° Decoderï¼ŒDecoder çš„è¾“å‡ºé€šå¸¸ä¼šå†ç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆå…·ä½“å–å†³äºä»»åŠ¡è¦æ±‚ï¼‰ã€‚
<div style="text-align: center;">
  <img src="./images/transformer.png" alt="Transformer" style="width: 300px; height: auto;">
</div>

Encoder å’Œ Decoder åˆ†åˆ«ä½¿ç”¨äº†ä¸¤ç§ maskï¼Œ`src_mask` å’Œ `tgt_mask`ã€‚`src_mask` ç”¨äºé®ç›–æ‰€æœ‰çš„ PAD tokenï¼Œé¿å…å®ƒä»¬åœ¨ attention è®¡ç®—ä¸­äº§ç”Ÿå½±å“ã€‚`tgt_mask` é™¤äº†é®ç›–æ‰€æœ‰ PAD tokenï¼Œè¿˜è¦é˜²æ­¢æ¨¡å‹åœ¨è¿›è¡Œ next word prediction æ—¶è®¿é—®æœªæ¥çš„è¯ã€‚

### [Positional Encoding](./modules/layers.py)
ç”±äº Transformer ä¸åƒ RNN é‚£æ ·å…·æœ‰å¤©ç„¶çš„åºåˆ—ç‰¹æ€§ï¼Œåœ¨è®¡ç®— attention æ—¶ä¼šä¸¢å¤±é¡ºåºä¿¡æ¯ï¼Œå› æ­¤éœ€è¦å¼•å…¥ä½ç½®ç¼–ç ã€‚åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œä½ç½®ç¼–ç çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

- å¯¹äºå¶æ•°ç»´åº¦ï¼š
  ```math
   \text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
  ```

- å¯¹äºå¥‡æ•°ç»´åº¦ï¼š
  ```math
  \text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) 
  ```

ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯¹ div term å–æŒ‡æ•°å’Œå¯¹æ•°ï¼Œå³ï¼š
```math
\text{div-term} = 10000^{2i/d_{\text{model}}} = \exp\left(\frac{2i \cdot -\log(10000)}{d_{\text{model}}}\right)
```

ä½ç½®ç¼–ç å¯¹ä»»ä½•åºåˆ—éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤ positional encoding çš„ shape ä¸º `[seq_len, d_model]`ã€‚ç„¶åæ ¹æ®å¹¿æ’­æœºåˆ¶ä¸ shape ä¸º `[batch_size, seq_len, d_model]` çš„ input embedding ç›¸åŠ ï¼Œå¾—åˆ° Encoder çš„è¾“å…¥ï¼Œè®°ä½œ $x_0$ã€‚

## [Encoder](./modules/encoder.py)
Encoder åŒ…å«å¤šä¸ªç›¸åŒçš„å±‚ã€‚ä¸Šä¸€å±‚çš„è¾“å‡º $x_i$ ä»¥å¦‚ä¸‹é€”å¾„ç»è¿‡è¯¥å±‚ï¼ˆçœç•¥äº† dropoutï¼‰ï¼š
```python
# attention mechanism
residual = x
x = multihead_attention(q=x, k=x, v=x, mask=src_mask)
x = layer_norm(x + residual)

# position-wise feed forward
residual = x
x = feed_forward(x)
x = layer_norm(x + residual)
```

## [Attention](./modules/layers.py)
Attention çš„è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š
<div style="text-align: center;">
  <img src="./images/attention.png" alt="Attention" style="width: 400px; height: auto;">
</div>
åœ¨ Encoder çš„ self-attention ä¸­ï¼ŒKã€Qã€V å‡ä¸ºä¸Šä¸€å±‚çš„è¾“å‡ºç»è¿‡ä¸åŒçº¿æ€§å±‚å¾—åˆ°çš„ã€‚åœ¨ Decoder çš„ cross-attention ä¸­ï¼ŒK å’Œ V æ¥è‡ª Encoder æœ€åä¸€å±‚çš„è¾“å‡ºï¼Œè€Œ Q æ˜¯ Decoder ä¸Šä¸€å±‚çš„è¾“å‡ºã€‚

ä¸ºäº†ä½¿æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„ä¸åŒç‰¹å¾å­ç©ºé—´ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå°† shape ä¸º `[batch_size, seq_len, d_model]` çš„ Kã€Qã€V åˆ†ä¸º `[batch_size, seq_len, n_head, d_key]`ï¼Œå†äº¤æ¢ `seq_len` å’Œ `n_head` ä¸¤ä¸ªç»´åº¦ï¼Œä»¥ä¾¿è¿›è¡Œ attention æœºåˆ¶ä¸­çš„çŸ©é˜µä¹˜æ³•ã€‚è®¡ç®—äº† attention ä¹‹åå†å°†ç»“æœåˆå¹¶ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°ä¸è¾“å…¥ç›¸åŒçš„ç»´åº¦ã€‚ç®—æ³•çš„æµç¨‹å¦‚ä¸‹ï¼š
```python
# projection
K, Q, V = W_k(x), W_q(x), W_v(x)

# split
d_key = d_model // n_head
K, Q, V = (K, Q, V).view(batch_size, seq_len, n_head, d_key).transpose(1, 2)
out = scaled_dot_product_attention(K, Q, V)

# concatenate
out = out.transpose(1, 2).view(batch_size, seq_len, d_model)
out = W_cat(out)
```

Scaled Dot-Product Attention ç”¨å…¬å¼è¡¨ç¤ºä¸ºï¼š
```math
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_{key}}}\right) \cdot V
```

## [Decoder](./modules/decoder.py)
Decoder ç›¸è¾ƒäº Encoder é™¤äº†å¤šäº†ä¸€å±‚ cross-attention ä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨äº† masked multi-head attentionã€‚ç”±äºæ¨¡å‹åœ¨æ­¤å¤„ä¸èƒ½è®¿é—®æœªæ¥ä¿¡æ¯ï¼Œå› æ­¤è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿç§°ä¸º causal self-attentionã€‚
Decoder åŒæ ·åŒ…å«å¤šä¸ªç›¸åŒçš„å±‚ï¼ŒEncoder æœ€åä¸€å±‚çš„è¾“å‡º `enc` å’Œ Decoder ä¸Šä¸€å±‚çš„è¾“å‡º `dec` ä»¥å¦‚ä¸‹é€”å¾„ç»è¿‡è¯¥å±‚ï¼ˆçœç•¥äº† dropoutï¼‰ï¼š
```python
# causal self-attention
residual = dec
x = multihead_attention(q=dec, k=dec, v=dec, mask=tgt_mask)
x = layer_norm(x + residual)

# cross-attention
x = multihead_attention(q=x, k=enc, v=enc, mask=tgt_mask)
x = layer_norm(x + residual)

# position-wise feed forward
residual = x
x = feed_forward(x)
x = layer_norm(x + residual)
```

## Training Strategy
### Training Data and Batching
[Attention is all you need](https://arxiv.org/pdf/1706.03762) Sec 5.1 æåˆ°ï¼Œè®­ç»ƒé›†ä½¿ç”¨çš„æ˜¯ WMT 2014ï¼Œæ¯ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡æœ‰å¤§çº¦ 25k source tokens å’Œ 25k target tokensï¼Œç»“æœäº§ç”Ÿäº† 6,230 ä¸ªæ‰¹æ¬¡ã€‚å¹³å‡æ‰¹æ¬¡å¤§å°ä¸º 724ï¼Œå¹³å‡é•¿åº¦ä¸º 45 ä¸ª tokensã€‚è€ƒè™‘åˆ° GPU æ˜¾å­˜ä¸è¶³ï¼Œä¸ºäº†ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡éƒ½æœ‰è¶³å¤Ÿçš„ tokensï¼Œå› æ­¤éœ€è¦é‡‡å–æ¢¯åº¦ç´¯ç§¯ç­–ç•¥ï¼Œæ¯ `update_freq` è½®æ‰æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ã€‚

è®ºæ–‡è¿˜æåˆ°å¯¹ base transformer è¿›è¡Œäº† 100,000 æ¬¡è¿­ä»£è®­ç»ƒï¼Œè¿™åº”è¯¥å¯¹åº”äº 16 ä¸ª epochsã€‚

### Optimizer
[Attention is all you need](https://arxiv.org/pdf/1706.03762) Sec 5.3 æåˆ°ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨çš„æ˜¯ Adamï¼Œå‚æ•°ä¾æ¬¡ä¸º $\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-9}$ã€‚æ­¤å¤–ï¼Œæ ¹æ®å¦‚ä¸‹å…¬å¼ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜äº†å­¦ä¹ ç‡ï¼š

```lrate = d_{\mathrm{model}}^{-0.5}\cdot\min(step\_ num^{-0.5},step\_ num\cdot warmup\_ steps^{-1.5})```

è¿™ç›¸å½“äºåœ¨å‰ $warmup_steps$ è®­ç»ƒæ­¥éª¤ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œç„¶åæŒ‰æ­¥æ•°çš„å¹³æ–¹æ ¹å€’æ•°æ¯”ä¾‹é™ä½å­¦ä¹ ç‡ã€‚Transformer base è®­ç»ƒäº† 100,000 æ­¥ï¼Œåœ¨æ­¤è®¾ç½®ä¸‹ $warmup\_ steps = 4000$ã€‚å­¦ä¹ ç‡çš„å¯è§†åŒ–å¦‚ä¸‹æ‰€ç¤ºï¼š
<div style="text-align: center;">
  <img src="./images/lr.png" alt="Learning Rate" style="width: 500px; height: auto;">
</div>

### Label Smoothing
[Attention is all you need](https://arxiv.org/pdf/1706.03762) Sec 5.4 æåˆ°ä½¿ç”¨æ ‡ç­¾å¹³æ»‘æŠ€æœ¯è™½ç„¶ä¼šæŸå®³æ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œä½†å¯ä»¥ç•¥å¾®æå‡ BLEU å’Œå‡†ç¡®ç‡ã€‚æ ‡ç­¾å¹³æ»‘æ˜¯ [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567) ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨è®¡ç®—æŸå¤±æ—¶å¯¹ç›®æ ‡æ ‡ç­¾è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡åœ°é¢„æµ‹å•ä¸ªç±»åˆ«ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒå°†æ ‡ç­¾ä»ç¡¬æ ‡ç­¾ï¼ˆone-hot vectorï¼‰è½¬å˜ä¸ºè½¯æ ‡ç­¾ï¼ˆsoft labelsï¼‰ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸€äº›ä¸ç¡®å®šæ€§ã€‚

å‡è®¾æœ‰ä¸€ä¸ªç±»åˆ«æ•°ä¸º $C$ çš„åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬ $x$ï¼Œæ ‡ç­¾å¹³æ»‘åçš„ç›®æ ‡åˆ†å¸ƒ $y_{\text{smooth}}$ å®šä¹‰ä¸ºï¼š

```math
y_{\text{smooth}} = (1 - \epsilon) \cdot y_{\text{one-hot}} + (1-y_{\text{one-hot}})\cdot \frac{\epsilon}{C-1}
```

å…¶ä¸­ï¼Œ$\epsilon$ æ˜¯å¹³æ»‘å‚æ•°ï¼Œé»˜è®¤ä¸º 0.1ã€‚$y_{\text{one-hot}}$ æ˜¯åŸå§‹çš„ one-hot æ ‡ç­¾ã€‚

ä½ å¯ä»¥åœ¨ [config.py](./config.py) ä¸­ä¿®æ”¹ `eps_ls` æ§åˆ¶ $\epsilon$ çš„å¤§å°ã€‚å¦‚æœ $\epsilon=0$ åˆ™å°†

ç¦ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ã€‚

## Evaluation
ä¸ºäº†è¯„ä¼°æœºå™¨ç¿»è¯‘çš„æ•ˆæœï¼Œæœ¬å®ç°éµå¾ªäº† [Attention is all you need](https://arxiv.org/pdf/1706.03762) çš„è®¾ç½®ï¼Œä½¿ç”¨ [BLEU](https://aclanthology.org/P02-1040.pdf) åˆ†æ•°ã€‚å…·ä½“è¿‡ç¨‹æ˜¯ï¼Œå…ˆä½¿æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ç»è¿‡ transformer çš„å‰å‘è¿‡ç¨‹ï¼Œç„¶åä½¿ç”¨ greedy decode çš„æ–¹æ³•ä» decoder è¾“å‡ºä¸­é€‰å–æ¦‚ç‡æœ€å¤§çš„ token ä½œä¸ºé¢„æµ‹ç»“æœã€‚ç„¶ååˆ©ç”¨ [sacrebleu](https://github.com/mjpost/sacrebleu) è®¡ç®— BLEUã€‚

ä¸ºäº†æé«˜ç¿»è¯‘çš„æ•ˆæœï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥ä½¿ç”¨ beam search ä½œä¸º decode æ–¹æ³•ï¼Œæ¬¢è¿æäº¤ PR :)ã€‚