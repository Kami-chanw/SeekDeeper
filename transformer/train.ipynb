{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data\n",
    "It contians following steps:\n",
    "1. Use tokenizers from `spacy` to tokenize texts from train dataset. \n",
    "2. Build the vocabulary, i.e. the tokens for the index dictionary. A list of special tokens (e.g. `<eos>`, `<pad>`) is prepended to the entire table.\n",
    "3. Prepare dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import load_data\n",
    "from modules import Transformer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "src_lang = \"en\"\n",
    "tgt_lang = \"de\"\n",
    "\n",
    "src_vocab, tgt_vocab, train_dataloader, valid_dataloader, test_dataloader = (\n",
    "    load_data(src_lang, tgt_lang)\n",
    ")\n",
    "\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "config.device = torch.device(\"cuda:3\")\n",
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Translation Model\n",
    "In standard implementations, there are usually no pre-set layers after the decoder. This means that for translation tasks, an additional linear layer needs to be added after the decoder to map the decoder output to the target vocabulary to obtain logits. However, for simplicity, the linear layer has been added to the decoder in this code implementation (see [this](./modules/decoder.py#89))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_pad_idx=src_vocab[\"<pad>\"],\n",
    "    tgt_pad_idx=tgt_vocab[\"<pad>\"],\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    d_model=config.d_model,\n",
    "    n_head=config.n_head,\n",
    "    max_len=config.max_len,\n",
    "    ffn_hidden=config.ffn_hidden,\n",
    "    n_layer=config.n_layer,\n",
    "    dropout=config.dropout,\n",
    "    device=config.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "Before we officially start training, in order to follow the settings in the paper \"[Attention is all you need](https://arxiv.org/pdf/1706.03762)\", we need to do the following steps:\n",
    "1. Define a custom learning rate scheduler that uses a warmup strategy. (Sec. 5.3)\n",
    "2. Rewrite the training objective to use label smoothing (Sec. 5.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,\n",
    "    betas=config.betas,\n",
    "    eps=config.adam_eps,\n",
    ")\n",
    "\n",
    "\n",
    "def lr_lambda(step):\n",
    "    return config.d_model**-0.5 * min(\n",
    "        (step + 1) ** -0.5, (step + 1) * config.warmup_step**-1.5\n",
    "    )\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        smoothing = config.eps_ls\n",
    "        pad_idx = self.tgt_vocab[\"<pad>\"]\n",
    "        classes = len(self.tgt_vocab)\n",
    "\n",
    "        if smoothing == 0:\n",
    "            return F.cross_entropy(pred, target, ignore_index=pad_idx)\n",
    "\n",
    "        log_prb = F.log_softmax(pred, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
    "            one_hot = one_hot * (1 - smoothing) + (1 - one_hot) * smoothing / (\n",
    "                classes - 1\n",
    "            )\n",
    "            mask = torch.nonzero(target == pad_idx)\n",
    "            if mask.dim() > 0:\n",
    "                one_hot.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        return torch.mean(torch.sum(-one_hot * log_prb, dim=-1))\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tgt_vocab[\"<pad>\"], label_smoothing=config.eps_ls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the `evaluate` function to evaluate the model's progress during training. Specifically, the loss and BLEU score are calculated on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_reverse = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "\n",
    "def split_batch(batch):\n",
    "    src, tgt = batch\n",
    "    src, tgt = src.transpose(0, 1), tgt.transpose(0, 1)\n",
    "    tgt, gt = tgt[:, :-1], tgt[:, 1:]\n",
    "    return src.to(config.device), tgt.to(config.device), gt.to(config.device)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_references = []\n",
    "    all_predictions = []\n",
    "    special_index = [\n",
    "        tgt_vocab[\"<pad>\"],\n",
    "        tgt_vocab[\"<sos>\"],\n",
    "        tgt_vocab[\"<eos>\"],\n",
    "        tgt_vocab[\"<unk>\"],\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=\"Evaluating\"):\n",
    "            src, tgt, gt = split_batch(batch)\n",
    "            outputs = model(src, tgt)\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, len(tgt_vocab))\n",
    "            gt = gt.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(outputs, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = outputs.view(src.size(0), -1, len(tgt_vocab))\n",
    "            pred_tokens = torch.argmax(outputs, dim=-1)\n",
    "            for pred, target in zip(pred_tokens, gt.view(src.size(0), -1)):\n",
    "                pred_sentence = [\n",
    "                    target_vocab_reverse[idx.item()]\n",
    "                    for idx in pred\n",
    "                    if idx.item() not in special_index\n",
    "                ]\n",
    "                target_sentence = [\n",
    "                    target_vocab_reverse[idx.item()]\n",
    "                    for idx in target\n",
    "                    if idx.item() not in special_index\n",
    "                ]\n",
    "                if pred_sentence and target_sentence:\n",
    "                    all_predictions.append(\" \".join(pred_sentence))\n",
    "                    all_references.append([\" \".join(target_sentence)])\n",
    "\n",
    "    avg_loss = total_loss / len(valid_dataloader)\n",
    "    if len(all_predictions) > 0:\n",
    "        bleu_score = sacrebleu.corpus_bleu(all_predictions, all_references)\n",
    "        avg_bleu = bleu_score.score\n",
    "    else:\n",
    "        avg_bleu = 0\n",
    "    return avg_loss, avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 227/227 [04:55<00:00,  1.30s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss:  10.0295, Validation Loss: 10.0448, BLEU Score: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 227/227 [04:51<00:00,  1.28s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Training Loss:  9.9448, Validation Loss: 9.9115, BLEU Score: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 227/227 [04:50<00:00,  1.28s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Training Loss:  9.7795, Validation Loss: 9.6949, BLEU Score: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 227/227 [04:49<00:00,  1.28s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Training Loss:  9.5517, Validation Loss: 9.4237, BLEU Score: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 227/227 [04:55<00:00,  1.30s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Training Loss:  9.2978, Validation Loss: 9.1561, BLEU Score: 2.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 227/227 [04:50<00:00,  1.28s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss:  9.0662, Validation Loss: 8.9423, BLEU Score: 3.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 227/227 [04:53<00:00,  1.29s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Training Loss:  8.8849, Validation Loss: 8.7767, BLEU Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 227/227 [04:48<00:00,  1.27s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Training Loss:  8.7474, Validation Loss: 8.6396, BLEU Score: 12.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 227/227 [04:45<00:00,  1.26s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Training Loss:  8.6336, Validation Loss: 8.5166, BLEU Score: 5.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 227/227 [04:53<00:00,  1.29s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Training Loss:  8.5286, Validation Loss: 8.4007, BLEU Score: 2.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 227/227 [04:49<00:00,  1.27s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Training Loss:  8.4190, Validation Loss: 8.2903, BLEU Score: 1.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 227/227 [04:59<00:00,  1.32s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss:  8.3006, Validation Loss: 8.1808, BLEU Score: 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 227/227 [04:54<00:00,  1.30s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:12<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Training Loss:  8.1765, Validation Loss: 8.0668, BLEU Score: 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 227/227 [05:02<00:00,  1.33s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss:  8.0494, Validation Loss: 7.9497, BLEU Score: 2.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 227/227 [05:05<00:00,  1.35s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Training Loss:  7.9175, Validation Loss: 7.8313, BLEU Score: 2.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 227/227 [05:38<00:00,  1.49s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:15<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Training Loss:  7.7867, Validation Loss: 7.7229, BLEU Score: 2.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 227/227 [06:23<00:00,  1.69s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss:  7.6661, Validation Loss: 7.6244, BLEU Score: 2.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 227/227 [06:06<00:00,  1.62s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:13<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Training Loss:  7.5589, Validation Loss: 7.5336, BLEU Score: 2.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 227/227 [06:07<00:00,  1.62s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:13<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Training Loss:  7.4631, Validation Loss: 7.4539, BLEU Score: 3.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 227/227 [05:59<00:00,  1.59s/it]\n",
      "Evaluating: 100%|██████████| 8/8 [00:11<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss:  7.3805, Validation Loss: 7.3826, BLEU Score: 3.41\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        # tgt: the input of decoder\n",
    "        # gt (ground truth): the training target\n",
    "        src, tgt, gt = split_batch(batch)\n",
    "\n",
    "        gt = gt.contiguous().view(-1)\n",
    "        # [batch_size, seq_len, tgt_vocab_size]\n",
    "        outputs = model(src, tgt)\n",
    "        # [batch_size * seq_len, tgt_vocab_size]\n",
    "        outputs = outputs.contiguous().view(-1, len(tgt_vocab))\n",
    "        loss = criterion(outputs, gt)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.clip)\n",
    "        \n",
    "        if (step + 1) % config.update_freq == 0 or (step + 1) == len(train_dataloader):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    avg_train_loss = train(epoch)\n",
    "    avg_valid_loss, avg_bleu = evaluate()\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{config.epochs}, Training Loss: {avg_train_loss: .4f}, Validation Loss: {avg_valid_loss:.4f}, BLEU Score: {avg_bleu:.2f}\"\n",
    "    )\n",
    "    \n",
    "checkpoint_path = os.path.join(config.checkpoint_dir, f\"en_de.pth\")\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
