{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Modules and Data\n",
                "It contians following steps:\n",
                "1. Use tokenizers from `spacy` to tokenize texts from train test_dataset. \n",
                "2. Build the vocabulary, i.e. the tokens for the index dictionary. A list of special tokens (e.g. `<eos>`, `<pad>`) is prepended to the entire table.\n",
                "3. Prepare test_dataset and dataloader."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=3)"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from data import load_data, PAD_TOKEN\n",
                "import torch\n",
                "import config\n",
                "import os\n",
                "\n",
                "src_lang = \"en\"\n",
                "tgt_lang = \"de\"\n",
                "\n",
                "src_tokenizer, tgt_tokenizer, test_loader = load_data(src_lang, tgt_lang, [\"test\"])\n",
                "device = torch.device(\"cuda\")\n",
                "dataset = test_loader.dataset\n",
                "\n",
                "device  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from modules import Transformer\n",
                "\n",
                "model = Transformer(\n",
                "    src_pad_idx=src_tokenizer.token_to_id(PAD_TOKEN),\n",
                "    tgt_pad_idx=tgt_tokenizer.token_to_id(PAD_TOKEN),\n",
                "    src_vocab_size=src_tokenizer.get_vocab_size(),\n",
                "    tgt_vocab_size=tgt_tokenizer.get_vocab_size(),\n",
                "    d_model=config.d_model,\n",
                "    n_head=config.n_head,\n",
                "    max_len=config.max_len,\n",
                "    ffn_hidden=config.ffn_hidden,\n",
                "    n_layer=config.n_layer,\n",
                "    dropout=config.dropout,\n",
                "    device=device,\n",
                ")\n",
                "state_dict = torch.load(config.checkpoint_dir / \"en_de_20.pth\")\n",
                "model.load_state_dict(state_dict[\"model\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Inference\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1mThe 1th source sentence\u001b[0m: And that's architecture.\n",
                        "\u001b[1mGround Truth\u001b[0m: Und das ist Architektur.\n",
                        "\u001b[1mgreedy-search\u001b[0m: Das ist Architektur .\n",
                        "\u001b[1msample\u001b[0m: Das ist das Museum .\n",
                        "\n",
                        "\u001b[1mThe 2th source sentence\u001b[0m: So what happened on this day?\n",
                        "\u001b[1mGround Truth\u001b[0m: Was ist an diesem Tag geschehen?\n",
                        "\u001b[1mgreedy-search\u001b[0m: Was passierte also mit diesem Film ?\n",
                        "\u001b[1msample\u001b[0m: Was passierte als Nächstes ?\n",
                        "\n",
                        "\u001b[1mThe 3th source sentence\u001b[0m: I also love the fact that if I wanted to walk from my house to New York City, I could.\n",
                        "\u001b[1mGround Truth\u001b[0m: Ebenso liebe ich die Tatsache, dass ich von meinem Haus nach New York City laufen könnte, wenn ich wollte.\n",
                        "\u001b[1mgreedy-search\u001b[0m: Ich dachte , ich würde gerne meine eigene Arbeit auf dem Schreibtisch machen , als ich in London reiste .\n",
                        "\u001b[1msample\u001b[0m: Ich dachte , wenn ich mir meinen Platz überlassen könnte , konnte ich in New York City fahren .\n",
                        "\n",
                        "\u001b[1mThe 4th source sentence\u001b[0m: That's the consequence that we have to face.\n",
                        "\u001b[1mGround Truth\u001b[0m: Das ist die Konsequenz, der wir uns stellen müssen.\n",
                        "\u001b[1mgreedy-search\u001b[0m: Das ist der Grund , dass wir uns alle vorstellen .\n",
                        "\u001b[1msample\u001b[0m: Das ist die Tatsache , die wir uns selbst vorstellen .\n",
                        "\n",
                        "\u001b[1mThe 5th source sentence\u001b[0m: And I always just say, \"Oh, I was scouted,\" but that means nothing.\n",
                        "\u001b[1mGround Truth\u001b[0m: Ich sage immer: \"Oh, ich wurde entdeckt,\" aber das bedeutet gar nichts.\n",
                        "\u001b[1mgreedy-search\u001b[0m: Und ich sagte : \" Ich bin hier , aber ich bin mir nicht sicher , dass es so einfach ist , aber ich bin sehr aufgeregt .\n",
                        "\u001b[1msample\u001b[0m: Und ich sagte : \" Ja , nein , ich bin schon naiv . Aber so ist es nicht .\n",
                        "\n",
                        "\u001b[1mBLEU score for greedy-search\u001b[0m: 55.07\n",
                        "\u001b[1mBLEU score for sample\u001b[0m: 25.41\n"
                    ]
                }
            ],
            "source": [
                "from utils import translate_sentence\n",
                "import sacrebleu\n",
                "\n",
                "num_sample = 5\n",
                "samples = dataset[torch.randint(0, len(dataset), (num_sample,))]\n",
                "\n",
                "method = {\n",
                "    \"greedy-search\": {\"num_beams\": 1, \"do_sample\": False},\n",
                "    \"sample\": {\n",
                "        \"num_beams\": 1,\n",
                "        \"do_sample\": True,\n",
                "        \"top_k\": config.top_k,\n",
                "        \"top_p\": config.top_p,\n",
                "        \"temperature\": config.temperature,\n",
                "    },\n",
                "}\n",
                "\n",
                "pred = {\n",
                "    method_name: translate_sentence(\n",
                "        samples[src_lang], model, src_tokenizer, tgt_tokenizer, **args\n",
                "    )\n",
                "    for method_name, args in method.items()\n",
                "}\n",
                "\n",
                "references = [[sentence] for sentence in samples[tgt_lang]]\n",
                "\n",
                "# Calculate BLEU scores for each method\n",
                "bleu_scores = {\n",
                "    method_name: sacrebleu.corpus_bleu(\n",
                "        pred_list, references\n",
                "    ).score\n",
                "    for method_name, pred_list in pred.items()\n",
                "}\n",
                "\n",
                "for i in range(num_sample):\n",
                "    print(f\"\\033[1mThe {i+1}th source sentence\\033[0m: {''.join(samples[src_lang][i])}\")\n",
                "    print(f\"\\033[1mGround Truth\\033[0m: {''.join(samples[tgt_lang][i])}\")\n",
                "    for method_name in method.keys():\n",
                "        print(f\"\\033[1m{method_name}\\033[0m: {pred[method_name][i]}\")\n",
                "    print()\n",
                "\n",
                "# Print BLEU scores\n",
                "for method_name, score in bleu_scores.items():\n",
                "    print(f\"\\033[1mBLEU score for {method_name}\\033[0m: {score:.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
